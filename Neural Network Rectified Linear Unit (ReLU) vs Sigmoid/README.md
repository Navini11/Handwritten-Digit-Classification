# Neural Network Rectified Linear Unit (ReLU) vs Sigmoid

#### ðŸ“Œ ```Neural Network Rectified Linear Unit (ReLU) vs Sigmoid.ipynb``` for the same task. Here we compare ```Sigmoid```, ```Relu``` functions' Training Loss, and Validation Loss.

### Objective for this Notebook

1. Define Several Neural Network, Criterion function, Optimizer.

2. Test Sigmoid and Relu.

3. Analyze Results.

### Table of Contents

In this lab, you will test Sigmoid and Relu activation functions on the MNIST dataset with two hidden Layers.

- Neural Network Module and Training Function
- Make Some Data
- Define Several Neural Network, Criterion function, Optimizer
- Test Sigmoid and Relu
- Analyze Results
